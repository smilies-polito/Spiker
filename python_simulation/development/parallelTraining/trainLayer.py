#!/Users/alessio/anaconda3/bin/python3

import numpy as np


# Function which updates an entire layer of spiking neurons in parallel. In particular
# it computes the new values of their membrane potentials and generates the output
# spikes in case some of them exceed the threshold.
#
# INPUT PARAMETERS:
#
# 	1) inEvents: boolean NumPy array containing the input of the current layer 
# 	   corresponding to the current step.
#
# 	2) layerDict: dictionary containing the parameters of all the neurons inside the
# 	   layer
#
# 	3) dt_tau: parameter that is common to all the neurons inside the network. It
# 	   represents the ratio delta_t/tau, where delta_t is the time duration of
# 	   the elaboration step and tau is time constant of the exponential. Being the
# 	   exponential function comupted step by step this parameter affects its
# 	   steepness. An higher value of delta_t/tau implies a faster decay of the
# 	   exponential function.
#
# 	4) v_reset: voltage at which the membrane potential is reset in case it exceeds
# 	   the threshold.
#
# 	5) currentStep: current elaboration step. Needed to compute time differences.
# 	   Using a loop to make the network evolve in time this simply corresponds to 
# 	   the value of the index used for the loop.

def updateLayer(inEvents, layerDict, dt_tau, v_reset, currentStep):

	# Neurons that have generated an output spike
	generateOutputEvents(layerDict)

	updateOutTime(layerDict, currentStep)

	ltp(layerDict, A_ltp, dt_tau, currentStep)

	resetPotentials(layerDict, v_reset)

	# Neurons that have not generated an output spike
	ltd(inEvents, layerDict, A_ltd, dt_tau, currentStep)


	# All the neurons
	expDecay(layerDict, dt_tau)

	updateInTime(inEvents, layerDict, currentStep)

	updateMembranePotentials(inEvents, layerDict)






# Generate the output spikes for all those neurons whose membrane potential exceeds the
# threshold.
#
# INPUT PARAMETER: layerDict. It is a dictionary containing the parameters of the
# layer, in particular:
#y
# 	1) v_mem: NumPy array containing the current value of the membrane potential of
# 	   each neuron in the layer.
#
# 	2) v_th: NumPy array containing the value of the threshold voltage for each
# 	   neuron in the layer. 
#
# 	3) outEvents: boolean NumPy array containing the spikes generated by the layer.

def generateOutputEvents(layerDict):
	layerDict["outEvents"] = layerDict["v_mem"] > layerDict["v_th"]







# Update the time instant in which the neurons have generated a spike.
#
# INPUT PARAMETERS: 
#
# 	1) layerdict: dictionary containing the parameters of the layer. in particular:
#
# 		a) t_out: NumPy array containing the instants of the last spike generated 
# 		   by each neuron in the layer.
#
# 		b) outEvents: boolean numpy array containing the spikes generated by the 
# 		   layer.
#
# 	2) currentStep: current value of the network training loop index. It corresponds
# 	   to the time, expressed in training steps, passed since the beginning of the
# 	   training loop. The output time is updated using currentStep-1 because the
# 	   membrane potential has been updated in the previous cycle, here it is only
# 	   evaluated.

def updateOutTime(layerDict, currentStep):
	layerDict["t_out"][layerDict["outEvents"]] = currentStep-1



# Update the time instant in which the synapses have brought an input spike.
#
# INPUT PARAMETERS:
#
# 	1) inEvents: boolean NumPy array containing the input spikes.
#
# 	2) layerDict: dictionary containing the parameters of the layer. In particular
# 	   t_in, a NumPy array containing the input time instants.
#
# 	3) currentStep: current elaboration step. Using a loop to make the network 
# 	   evolve in time this simply corresponds to the value of the index used for the 
# 	   loop. Needed to update the input time instants with the current
# 	   elaboration step.


def updateInTime(inEvents, layerDict, currentStep):
	layerDict["t_in"][inEvents] = currentStep






# LTP (long term plasticity): increase the weights of all the input synapses of those 
# neurons that have generated an output event, following the equation
#
# 			w' = w + A_ltp*e^(-Dt_tau)
#
# INPUT PARAMETERS:
#
# 	1) layerDict: dictionary containing the parameters of the layer. in particular:
#
# 		a) weights: bidimensional NumPy array containing the weights of all the
# 		   synapses connected to each neuron in the layer.
#
# 		b) outEvents: boolean numpy array containing the spikes generated by the 
# 		   layer.
#
# 	2) A_ltp: constant which affects the weights increment due to the STDP.
#
# 	3) dt_tau: parameter which represents the ratio between a single elaboration step
# 	   and the exponential time constant. Whichever ratio Delta_t/tau can be expressed
# 	   as a multiple of this quantity. Delta_t in this case represents the difference
# 	   between when the neuron has generated an output event and when the various
# 	   inputs have generated the last spike.
#
# 	4) currentStep: current elaboration step. Using a loop to make the network evolve
# 	   in time this simply corresponds to the value of the index used for the loop.
# 	   In the current elaboration step the membrane potential is checked to determine
# 	   wether it has exceeded the threshold or not, but the update of its value was
# 	   performed in the previous cycle. This means that if a specific neurons
# 	   has generated an event the time instant of this event correspond to the
# 	   previous elaboration step.

def ltp(layerDict, A_ltp, dt_tau, currentStep):

	# Subtract each input time from the output time
	t_out = currentStep - 1
	timeSteps = t_out - layerDict["t_in"]

	# Compute the ltp increase
	Dt_tau = timeSteps*dt_tau
	plasticity = A_ltp*np.exp(-Dt_tau)

	# Increase the weights of the active neurons
	layerDict["weights"][layerDict["outEvents"]] += plasticity







# LTD (long term depression): decrease the weights of the active input synapses of those
# neurons that have not generated any outout event, following the equation
#
# 			w' = w + A_ltd*e^(Dt_tau)
#
# INPUT PARAMETERS:
#
#	1) inEvents: boolean NumPy array which contains the input spikes for the current.
#
# 	2) layerDict: dictionary containing the parameters of the layer. in particular:
#
# 		a) weights: bidimensional NumPy array containing the weights of all the
# 		   synapses connected to each neuron in the layer.
#
# 		b) outEvents: boolean numpy array containing the spikes generated by the 
# 		   layer.
#
# 	3) A_ltd: constant which affects the weights decrease due to the STDP.
#
# 	4) dt_tau: parameter which represents the ratio between a single elaboration step
# 	   and the exponential time constant. Whichever ratio Delta_t/tau can be expressed
# 	   as a multiple of this quantity. Delta_t in this case represents the difference
# 	   between when the inputs have generated an event and when the neuron has fired for
# 	   the last time
#
# 	5) currentStep: current elaboration step. Using a loop to make the network evolve
# 	   in time this simply corresponds to the value of the index used for the loop.
# 	   Needed to compute the time difference between the input events generated in the
# 	   current cycle and the output events of the previous cycles.

def ltd(inEvents, layerDict, A_ltd, dt_tau, currentStep):
	
	# Compute the time difference for all the neurons
	timeSteps = layerDict["t_out"] - currentStep

	# Compute the ltd depression
	Dt_tau = timeSteps*dt_tau
	depression = A_ltd*np.exp(Dt_tau)

	# Create a bidimensional array with values different from 0 only where an input
	# event has been generated
	activeSynapses = np.outer(depression, inEvents)

	# Update only the weights of the inactive neurons
	inactiveNeurons = np.logical_not(layerDict["outEvents"])

	layerDict["weights"][inactiveNeurons] += activeSynapses[inactiveNeurons]








# Reset the membrane potential of all those neurons whose membrane potential has
# exceeded the threshold. The function gives the possibility to chose the value of the
# reset voltage.
#
# INPUT PARAMETERS:
#
# 	1) layerDict: dictionary containing the parameters of the layer.
#
# 	2) v_reset: value at which the membrane potential is reset.

def resetPotentials(layerDict, v_reset):
	layerDict["v_mem"][layerDict["outEvents"]] = v_reset









# Update the membrane potential of all the neurons inside the layer with an exponential
# decay.
#
# INPUT PARAMENTERS: 
#
# 	1) layerDict: dictionary containing the parameters of the layer, in particular 
# 	   v_mem, NumPy array containing the current value of the membrane potential of 
# 	   each neuron in the layer. 
#
# 	2) dt_tau: parameter that is common to all the neurons inside the network. It
# 	   represents the ratio delta_t/tau, where delta_t is the time duration of
# 	   the elaboration step and tau is time constant of the exponential. Being the
# 	   exponential function comupted step by step this parameter affects its
# 	   steepness. An higher value of delta_t/tau implies a faster decay of the
# 	   exponential function.

def expDecay(layerDict, dt_tau):
	layerDict["v_mem"] = layerDict["v_mem"]-dt_tau*layerDict["v_mem"]







# Update the membrane potential of all those neurons which have received a spike in input.
#
# INPUT PARAMETERS:
#
# 	1) inEvents: boolean NumPy array which contains True whenever a spike has been
# 	   received in the  specific position and False otherwise.
#
# 	2) layerDict: dictionary containing all the parameters of the layer. In particular:
#	
#		a) v_mem: NumPy array containing the current value of the membrane potential of
#		   each neuron inside the layer.
#
# 		b) weights: bidimensional NumPy array which contains the weights of all the
# 		   neurons in the layer. It contains a number of arrays equal to the number of
# 		   neurons inside the layer. Each array contains a number of weights equal to the
# 		   number of input events, that is the number of neurons in the previous layer.

def updateMembranePotentials(inEvents, layerDict):
	layerDict["v_mem"] = layerDict["v_mem"] + np.sum(layerDict["weights"][:,inEvents], 
				axis=1)
