#!/Users/alessio/anaconda3/bin/python3

import numpy as np


def updateLayer(inEvents, layerDict, v_mem_dt_tau, stdp_dt_tau, v_reset, A_ltp,
			A_ltd, currentStep):

	'''
	One training step for a single layer.

	INPUT:

		1) inEvents: boolean NumPy array containing the input of the
		current layer corresponding to the current step.

		2) layerDict: dictionary containing the parameters of all the
		neurons inside the layer

		3) v_mem_dt_tau: ratio of the time step and the membrane
		exponential time constant.

		4) stdp_dt_tau: ratio of the time step and the stdp
		exponential time constant.

		5) v_reset: float. Reset value for the membrane potential.

		6) A_ltp: starting point of the "pre" exponential.

		7) A_ltd: starting point of the "post" exponential.

		8) currentStep: current value of the network training loop
		index. 
	'''

	# Generate output spike if the membrane potential exceeds the threshold
	generateOutputSpikes(layerDict)

	# Update output time for the active neurons
	updateOutTime(layerDict, currentStep)

	# Update the input and output masks
	updateInOutMasks(layerDict, inEvents)

	# Reset the potential of the active neurons
	resetPotentials(layerDict, v_reset)

	# Update the weights through STDP
	stdp(layerDict, A_ltp, A_ltd, stdp_dt_tau, currentStep)

	# Decrease the membrane potential through exponential decay
	expDecay(layerDict, v_mem_dt_tau)

	# Update the input time for the active synapses
	updateInTime(inEvents, layerDict, currentStep)

	# Update the membrane potential with the input spikes
	updateMembranePotentials(inEvents, layerDict)







def generateOutputSpikes(layerDict): 

	''' 
	Generate the output spikes for all those neurons whose membrane
	potential exceeds the threshold.

	INPUT: 

		layerDict: dictionary containing the parameters of the layer, in
		particular:

			a) v_mem: NumPy array containing the current value of
			the membrane potential of each neuron in the layer.

			b) v_th: NumPy array containing the value of the
			threshold voltage for each neuron in the layer. 

			c) outEvents: boolean NumPy array containing the spikes
			generated by the layer.  
	'''

	layerDict["outEvents"] = layerDict["v_mem"] > layerDict["v_th"]





	
def updateOutTime(layerDict, currentStep):

	''' 
	Update the time instant in which the neurons have generated a spike.

	INPUT PARAMETERS: 

		1) layerdict: dictionary containing the parameters of the layer,
		in particular:

			a) t_out: NumPy array containing the instants of the
			last spike generated by each neuron in the layer.

			b) outEvents: boolean numpy array containing the spikes
			generated by the layer.

		2) currentStep: current value of the network training loop
		index. 

	'''

	layerDict["t_out"][layerDict["outEvents"]] = currentStep - 1






def updateInOutMasks(layerDict, inEvents):

	''' Update the input and output masks.

	INPUT: 
		1) layerDict: dictionary containing the parameters of all the
		neurons inside the layer. 

		2) inEvents: boolean NumPy array containing the input of the
		current layer corresponding to the current step.
	'''

	# Update the mask of neurons that have already been active
	updateMask(layerDict, "outMask", layerDict["outEvents"])

	# Update the mask of inputs that have already been active
	updateMask(layerDict, "inMask", inEvents)





def updateMask(layerDict, maskName, spikeArray): 

	''' 
	Update the mask which allows to select only the elements that have
	already been active.

	INPUT:

		1) layerDict: dictionary containing the parameters of all the
		neurons inside the layer.

		2) maskName: string containing the mask to modify. This must
		correspond to one of the two mask names used as keys of
		layerDict.

		3) spikeArray: array of spikes used to update the mask.  
	'''

	layerDict[maskName] = np.logical_or(layerDict[maskName], spikeArray)





def resetPotentials(layerDict, v_reset):

	''' 
	Reset the membrane potential of all those neurons whose membrane
	potential has exceeded the threshold. The function gives the possibility
	to chose the value of the reset voltage.

	INPUT:

		1) layerDict: dictionary containing the parameters of the layer.

		2) v_reset: value at which the membrane potential is reset.  
	'''

	layerDict["v_mem"][layerDict["outEvents"]] = v_reset






def stdp(layerDict, A_ltp, A_ltd, dt_tau, currentStep):

	'''
	Training step through STDP.

	INPUT:
		1) layerDict: dictionary containing the parameters of the layer.

		2)  A_ltp: constant which affects the weights increment due to
		the STDP.

		3) A_ltd: constant which affects the weights decrease due to the
		STDP.

		4) dt_tau: ratio of the time step and the stdp exponential time 
		constant.

		5) currentStep: current value of the network training loop
		index.  
	'''

	ltp(layerDict, A_ltp, dt_tau, currentStep)

	ltd(inEvents, layerDict, A_ltd, dt_tau, currentStep)






def ltp(layerDict, A_ltp, dt_tau, currentStep):

	''' 
	Increase the weights of the already active synapses through LTP for
	the active neurons.

	INPUT:

		1) layerDict: dictionary containing the parameters of the layer.
		in particular:

			a) "weights": two-dimensional NumPy array containing the
			weights of all the synapses connected to each neuron in
			the layer.

			b) "outEvents": boolean numpy array containing the
			spikes generated by the layer.

		2) A_ltp: constant which affects the weights increment due to
		the STDP.

		3) dt_tau: ratio of the time step and the exponential time
		constant.

		4) currentStep: current value of the network training loop
		index.  
	'''


	# Subtract each input time from the output time
	t_out = currentStep - 1 
	timeSteps = t_out - layerDict["t_in"]

	# Compute the ltp increase for all the inputs
	Dt_tau = timeSteps*dt_tau 
	plasticity = A_ltp*np.exp(-Dt_tau)

	# Select only the inputs that have already received an input
	plasticity = plasticity*layerDict["inMask"]

	# Increase the weights of the active neurons
	layerDict["weights"][layerDict["outEvents"]] += plasticity




	

def ltd(inEvents, layerDict, A_ltd, dt_tau, currentStep):

	''' 
	Decrease the weights of the active synapse for the inactive neurons
	that have already generated a spike in their history.

	INPUT:

		1) inEvents: boolean NumPy array which contains the input spikes
		for the current.

		2) layerDict: dictionary containing the parameters of the layer.
		in particular:

			a) "weights": bidimensional NumPy array containing the
			weights of all the synapses connected to each neuron in
			the layer.

			b) "outEvents": boolean numpy array containing the
			spikes generated by the layer.

		3) A_ltd: constant which affects the weights decrease due to the
		STDP.

		4) dt_tau: ratio of the time step and the exponential time
		constant.

		5) currentStep: current value of the network training loop
		index.  
	'''

	
	# Compute the time difference for all the neurons
	timeSteps = layerDict["t_out"] - currentStep

	# Compute the ltd depression
	Dt_tau = timeSteps*dt_tau 
	depression = A_ltd*np.exp(Dt_tau)

	# Create a bidimensional array with values different from 0 only where
	# an input event has been generated
	activeSynapses = np.outer(depression, inEvents)

	# Update only the weights of the inactive neurons
	inactiveNeurons = np.logical_not(layerDict["outEvents"])

	# Select only the neurons that have already generated an output spike
	inactiveNeurons = np.logical_and(inactiveNeurons, layerDict["outMask"])


	layerDict["weights"][inactiveNeurons] += \
		activeSynapses[inactiveNeurons]






def updateInTime(inEvents, layerDict, currentStep):

	''' 
	Update the time instant in which the synapses have brought an input
	spike.

	INPUT PARAMETERS:

		1) inEvents: boolean NumPy array containing the input spikes.

		2) layerDict: dictionary containing the parameters of the layer.
		In particular t_in, a NumPy array containing the input time
		instants.

		3) currentStep: current value of the network training loop
		index.  
	'''

	layerDict["t_in"][inEvents] = currentStep








	
def expDecay(layerDict, dt_tau): 
	
	''' 
	Update the membrane potential of all the neurons inside the layer
	with an exponential decay.

	INPUT: 

		1) layerDict: dictionary containing the parameters of the layer,
		in particular:

			a) "v_mem", NumPy array containing the current value of
			the membrane potential of each neuron in the layer.

			b) "v_rest", float value corresponding to the rest
			potential.

		2) dt_tau: ratio of the time step and the exponential time
		constant.  
	'''

	layerDict["v_mem"] = layerDict["v_mem"] - dt_tau*(layerDict["v_mem"] -
				layerDict["v_rest"])






def updateMembranePotentials(inEvents, layerDict):

	''' 
	Update the membrane potential of all those neurons which have
	received a spike in input.

	INPUT PARAMETERS:

		1) inEvents: boolean NumPy array which contains True whenever a
		spike has been received in the  specific position and False
		otherwise.

		2) layerDict: dictionary containing all the parameters of the
		layer. In particular:
	      
		a) "v_mem": NumPy array containing the current value of the
		membrane potential of each neuron inside the layer.

			b) weights: bidimensional NumPy array which contains the
			weights of all the neurons in the layer.  
	'''

	layerDict["v_mem"] = layerDict["v_mem"] + np.sum(layerDict["weights"]\
				[:,inEvents], axis=1)


